---
layout: default
title: ISD 
---

<div style="height:25px;">
<p style="text-align:center;"><a href="https://www.csee.umbc.edu/~soroush">Soroush Abbasi Koohpayegani</a><sup>∗</sup>, <a href="">Ajinkya Tejankar</a><sup>∗</sup>, <a href="">Vipin Pillai</a>, <a href="">Paolo Favaro</a>, <a href="https://www.csee.umbc.edu/~hpirsiav/">Hamed Pirsiavash</a></p>
</div>
<div style="height:25px;">
<p style="text-align:center;">University of Maryland, Baltimore County, University of Bern</p>
</div>
<div style="height:30px;">
<p style="text-align:center; font-size:12px"><sup>∗</sup> denote equal contribution</p>
</div>

<div class="menu">
  <ul style="margin: 0px;">
      <li><a href='https://arxiv.org/abs/2010.14713'>[Paper]</a></li>
      <li><a href='.'>[Poster]</a></li>
      <li><a href='https://github.com/UMBCvision/ISD'>[Code]</a></li>
      <li><a href='/CompRess/bib.txt'>[Bib]</a></li>
  </ul>
</div>

<div>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/teaser.gif" width="100%" alt style></p>

<h5 id="abstract"><b>Abstract</b></h5>
<p>Recently, contrastive learning has achieved great resultsin self-supervised learning, where the main idea is to push two augmentations 
  of an image (positive pairs) closer compared to other random images (negative pairs).  We argue that not all random images are equal. 
  Hence, we introduce a  self  supervised  learning  algorithm  where  we  use  a  soft similarity for the negative images rather than a binary 
  distinction between positive and negative pairs. We iteratively distill a slowly evolving teacher model to the student model by capturing 
  the similarity of a query image to some random images and transferring that knowledge to the student. We argue that our method is 
  less constrained compared to recent  contrastive  learning  methods,  so  it  can  learn  better features.  
  Specifically, our method should handle unbalanced and unlabeled data better than existing contrastive learning  methods,  
  because  the  randomly  chosen  negative set might include many samples that are semantically similar  to  the  query  image. 
  In  this  case,  our  method  labels them as highly similar while standard contrastive methods label  them  as  negative  pairs.  
  Our  method  achieves  better results compared to state-of-the-art models like BYOL and MoCo on transfer learning settings. 
  We also show that our  method  performs  better  in  the  settings  where  the  unlabeled  data  is  unbalanced.</p>

<h5 id="contributions"><b>Contributions</b></h5>
<p> 
  ToDo: write our contribution
  
  
      We sample some query images randomly (left column), calculate their teacher probability distribution over all anchor points in the memory bank 
  (size=128K) and rank them in descending order (right columns). The second left column is another augmented version of the query image that contrastive
  learning methods use for the positive pair. Our students learns to mimic the probability number written below each anchor image while contrastive learning
  method (e.g., MoCo) learn to predict the one-hot encoding written below the images. Note that there are lots of images in the top anchor points that are 
  semantically similar to the query point that MoCo tries to discriminate them from the query while our method does not.
    </p>

<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/fig1.png" width="100%" alt style></p>
    
    <h5 id="Method"><b>Method</b></h5>
<p> 
    We initialize both teacher and student networks from scratch and update the teacher as running average of the student.
  We feed some random images to the teacher, and feed two different augmentations of a query image to both teacher and student. 
  We capture the similarity of the query to the anchor points in the teacher's embedding space and transfer that knowledge to 
  the student. We update the student based on KL divergence loss and update the teacher to be a slow moving average of the student.
  This can be seen as a soft version of MoCo which can handle negative images that are similar to the query
  image. Note that unlike contrastive learning and BYOL, we never compare two augmentations of the query images directly (positive pair)
    </p>

<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/teaser.gif" width="90%" alt style></p>

<h5 id="results"><b>Self-supervised Learning Results</b></h5>

    <p>
        We compare our method with other SSL methods on Linear and Nearest Neighbor (NN) evaluation on ImageNet as well as recall 
      on transfer learning settings. Our method (ISD) outperforms MoCo and BYOL in most evaluations. * denotes that CompRess is 
      not directly comparable as it uses a larger SSL model as the teacher: MoCo-V2(ResNet50) for ResNet18 and SimCLR(ResNet50x4) for ResNet50.
        
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table1.png" width="100%" alt style>
    
    </p>
    
    <p>
        Additionally, we evaluate our models on transfer tasks using nearest neighbor evaluation. * denotes that CompRess is not directly
      comparable as it uses a larger SSL model as the teacher: MoCo-V2(ResNet50) for ResNet18 and SimCLR(ResNet50x4) for ResNet50.
        
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table2.png" width="60%" alt style>
    
    </p>
  
  
  <h5 id="evolution"><b>Evolution of teacher and student models</b></h5>
  
    <p>
   Comparing the teacher and student ResNet18 models using Nearest Neighbor while training for MoCo, BYOL, and ISD methods. 
  Interestingly, the teacher performs better than the student before shrinking the learning rate. Most previous works use 
  the student as the final model which seems to be sub-optimal. We believe this is due to ensembling effect similar to [MeanTeacher] and needs more investigation.
    
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/fig2.png" width="100%" alt style>
    
    </p>

    
    <h5 id="augmentation biases"><b>Removing the bias of aggressive augmentations </b></h5>
    
<p>
       We use a variation of our method to remove the bias of aggressive augmentations that most recent SSL methods are trained with. 
  We believe the student model performs better than the teacher on transfer learning settings since it has learned less unnatural biases.  
  We use Linear evaluation and NN for evaluation.
        
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table4.png" width="100%" alt style>
    
    </p>
  
 
<h5 id="unbalanced dataset"><b>Self-Supervised  Learning  on  Unbalanced Dataset </b></h5>
    
<p>
         Nearest Neighbor (NN) results for the unbalanced data when we consider all 38 categories and 30 small categories separately. 
  We repeat the experiment 10 times with different random sets of 38 categories. NN is done on the validation set of ImageNet 
  (which has uniform distribution) by searching the nearest neighbors among all ImageNet training data of those 38 categories 
  (so the training data of NN also has uniform distribution). Hence, the whole evaluation is on balanced data to make sure we observe 
  the effect of the unbalanced, ``unlabeled'' data only. ``Diff'' shows the improvement of our method over MoCo. Interestingly the 
  improvement is bigger in the rare categories. This is aligned with out hypothesis that our method can handle unbalanced, unlabeled 
  data better since it does not consider all negative images equally negative.

        
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table5.png" width="90%" alt style>
    
    </p>
    
    


  
<h5 id="cluster"><b>Cluster Visualizations</b></h5>   
    
    <p> We cluster ImageNet dataset into 1000 clusters using k-means and show random samples from random clusters.  
      Each row corresponds to a cluster. Note that semantically similar images are clustered together.
    </p>    
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/clusters.jpg" width="100%" alt style></p>

<h5 id="references"><b>References</b></h5>
  [1] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Compress: Self-supervised learning by compressing representations. Advances in Neural Information Processing Systems, 33, 2020.
  [2] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-otr Bojanowski, and Armand Joulin.   Unsupervised learning of visual features by contrasting cluster assignments. arXivpreprint arXiv:2006.09882, 2020.
  [3] Jean-Bastien  Grill,  Florian  Strub,  Florent  Altche,  Corentin Tallec,  Pierre  H  Richemond,  Elena  Buchatskaya,  Carl  Doersch,  Bernardo Avila Pires,  Zhaohan Daniel Guo,  Mohammad Gheshlaghi Azar,  et al.   Bootstrap your own latent:  A new  approach  to  self-supervised  learning. arXiv  preprintarXiv:2006.07733, 2020.
  [4] Kaiming He,  Haoqi Fan,  Yuxin Wu,  Saining Xie,  and Ross Girshick.   Momentum  contrast  for  unsupervised  visual  representation learning.  InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729–9738, 2020.
  [5] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, ChenSun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie.  The inaturalist species classification and detectiondataset, 2018.
  [6] onathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3dobject representations for fine-grained categorization. InProceedings of the IEEE international conference on computervision workshops, pages 554–561, 2013.
  [7] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.Finegrained visual classification of aircraft. Technical report,2013.
  [8] Maria-Elena  Nilsback  and  Andrew  Zisserman.    Automated flower  classification  over  a  large  number  of  classes.   InIndian  Conference  on  Computer  Vision,  Graphics  and  Image Processing, Dec 2008.
  [9] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie,  and  P.  Perona.   Caltech-UCSD  Birds  200.   Technical  Report  CNS-TR-2010-001,  California  Institute  of  Technology, 2010.
  
  
  
